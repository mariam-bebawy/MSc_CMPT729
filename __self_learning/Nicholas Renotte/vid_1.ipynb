{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "trRImS3HQETY"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.3.0\n",
        "# !pip install tensorflow==2.12.0\n",
        "!pip install keras==2.10.0 tensorflow==2.10.0\n",
        "!pip install gym\n",
        "# !pip install keras\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup a random prebuilt environment\n",
        "\n",
        "# import dependencies\n",
        "import gym, random"
      ],
      "metadata": {
        "id": "DkHRdSEoQL4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "9ElWn6F2QYln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize what is actually happening\n",
        "# aka build the environment\n",
        "\n",
        "# env.step() returns 5 values instead of 4\n",
        "# terminated=True if environment terminates (eg. due to task completion, failure etc.)\n",
        "# truncated=True if episode truncates due to a time limit or a reason that is not defined as part of the task MDP.\n",
        "\n",
        "episodes = 10\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done_terminated = False\n",
        "    score = 0\n",
        "\n",
        "    while not done_terminated:\n",
        "        env.render()\n",
        "        action = random.choice([0, 1])\n",
        "        n_state, reward, done_terminated, info = env.step(action)\n",
        "        score += reward\n",
        "    print(f'episode: {episode}, score: {score}')"
      ],
      "metadata": {
        "id": "ZoRSZq9XQaMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "SpCqMrd6QdFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model that will learn to take the best action based on maximizing the reward\n",
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1, states)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "gxDCJ0iRQik8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(states, actions)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "w3R1j8bKQlz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "S7b6VNTeQnAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = BoltzmannQPolicy()\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "MeOjNBrSQohA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "id": "Zw5HauVpScro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "id": "3Nr0sKdmSnG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just to visualize\n",
        "_ = dqn.test(env, nb_episodes=15, visualize=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Nt42ayBjVjMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "FMCKjPgnVqS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del dqn\n",
        "del env"
      ],
      "metadata": {
        "id": "aFT8OOrNV05V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rebuild environment\n",
        "env = gym.make('CartPole-v0')\n",
        "actions = env.action_space.n\n",
        "states = env.observation_space.shape[0]\n",
        "\n",
        "# rebuild model\n",
        "model = build_model(states, actions)\n",
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "# reload weights\n",
        "dqn.load_weights('dqn_weights.h5f')\n",
        "\n",
        "# test out environment\n",
        "_ = dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "metadata": {
        "id": "oA99s4c5V0Ig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}